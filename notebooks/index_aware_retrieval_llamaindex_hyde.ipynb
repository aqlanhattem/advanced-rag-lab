{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index-aware Retrieval with LlamaIndex\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates advanced retrieval techniques for improving RAG (Retrieval-Augmented Generation) systems:\n",
    "\n",
    "- **Hypothetical Document Embedding (HyDE)**: Generate hypothetical answers to improve retrieval\n",
    "- **Query Expansion**: Add context and synonyms to queries for better matching\n",
    "- **Hybrid Search**: Combine semantic and keyword search\n",
    "- **Graph RAG**: Leverage knowledge graphs for structured retrieval\n",
    "\n",
    "## Dataset\n",
    "We'll use \"Anabasis of Alexander\" - a 2nd century historical account of Alexander the Great from Project Gutenberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies\n",
    "Install required packages and configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment to run)\n",
    "# %pip install --quiet llama-index llama-index-llms-gemini llama-index-embeddings-huggingface pydantic-ai\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"../keys.env\")\n",
    "\n",
    "# Verify API keys\n",
    "assert os.environ[\"GEMINI_API_KEY\"][:2] == \"AI\", \"Please specify GEMINI_API_KEY in keys.env file\"\n",
    "assert os.environ[\"HF_TOKEN\"][:2] == \"hf\", \"Please specify HF_TOKEN in keys.env file\"\n",
    "\n",
    "# Add path for custom modules\n",
    "# ------------------------------------------------------------------\n",
    "#  Minimal replacement for ../06_basic_rag/gutenberg_text_loader.py\n",
    "# ------------------------------------------------------------------\n",
    "import requests, pathlib, os\n",
    "\n",
    "def _download_anabasis():\n",
    "    \"\"\"Download Anabasis of Alexander (Gutenberg #46976) into ./.cache/pg46976.txt\"\"\"\n",
    "    cache_dir = pathlib.Path(\"./.cache\")\n",
    "    cache_dir.mkdir(exist_ok=True)\n",
    "    outfile = cache_dir / \"pg46976.txt\"\n",
    "\n",
    "    if outfile.exists():\n",
    "        print(\"ðŸ“‚ Text already present:\", outfile.resolve())\n",
    "        return\n",
    "\n",
    "    url = \"https://www.gutenberg.org/cache/epub/46976/pg46976.txt\"\n",
    "    print(\"â¬‡ï¸  Downloading Alexander text â€¦\")\n",
    "    resp = requests.get(url, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    outfile.write_text(resp.text, encoding=\"utf-8\")\n",
    "    print(\"âœ… Saved:\", outfile.resolve())\n",
    "\n",
    "# run once\n",
    "_download_anabasis()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = \"gemini-2.0-flash\"\n",
    "EMBED_MODEL_ID = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "print(\"âœ… Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plain Semantic Indexing (Baseline)\n",
    "Set up basic semantic indexing for comparison with advanced techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean previous runs (uncomment to start fresh)\n",
    "# !rm -rf .cache vector_index\n",
    "\n",
    "# Check existing indices\n",
    "# !ls ./.cache vector_index\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core import Document\n",
    "\n",
    "# Configure embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_ID)\n",
    "\n",
    "# Chunking configuration\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 20\n",
    "TOP_K = 2\n",
    "\n",
    "# Alternative fine-grained configuration\n",
    "# Settings.chunk_size = 100\n",
    "# Settings.chunk_overlap = 10\n",
    "# TOP_K = 4\n",
    "\n",
    "INDEX_DIR = \"vector_index\"\n",
    "\n",
    "# Load or create index\n",
    "if os.path.isdir(INDEX_DIR):\n",
    "    print(\"ðŸ“‚ Loading existing index...\")\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "else:\n",
    "    print(\"ðŸ“¥ Downloading and processing text...\")\n",
    "    # Download Alexander's Anabasis\n",
    "    gs = gtl.GutenbergSource()\n",
    "    doc = gs.load_from_url(\"https://www.gutenberg.org/cache/epub/46976/pg46976.txt\")\n",
    "    \n",
    "    # Load documents\n",
    "    documents = SimpleDirectoryReader(\n",
    "        input_dir=\"./.cache\", \n",
    "        required_exts=[\".txt\"], \n",
    "        exclude_hidden=False\n",
    "    ).load_data()\n",
    "    \n",
    "    # Create vector index\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=INDEX_DIR)\n",
    "    print(\"âœ… Index created and persisted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic RAG Setup\n",
    "Configure the basic RAG query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Initialize LLM\n",
    "llm = Gemini(model=f\"models/{MODEL_ID}\", api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Create query engine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=index.as_retriever(similarity_top_k=TOP_K),\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "def semantic_rag(question):\n",
    "    \"\"\"\n",
    "    Basic semantic RAG function.\n",
    "    \n",
    "    Args:\n",
    "        question: User query\n",
    "        \n",
    "    Returns:\n",
    "        Dict with answer and source nodes\n",
    "    \"\"\"\n",
    "    response = query_engine.query(question)\n",
    "    result = {\n",
    "        \"answer\": str(response),\n",
    "        \"source_nodes\": response.source_nodes\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ¤– Answer:\", result['answer'])\n",
    "    print(\"\\nðŸ“š Sources:\")\n",
    "    for i, node in enumerate(result['source_nodes'], 1):\n",
    "        print(f\"\\n{i}. Score: {node.score:.3f}\")\n",
    "        print(f\"   Text: {node.text[:200]}...\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Testing\n",
    "Test basic semantic RAG to understand limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: General Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” Test 1: How did Alexander treat conquered people?\")\n",
    "response = semantic_rag(\"How did Alexander treat the people of the places he conquered?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Specific Detail Question\n",
    "This tests the system's ability to find specific details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ” Test 2: Where did Alexander die?\")\n",
    "response = semantic_rag(\"Where did Alexander die?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding Chunk Size Impact\n",
    "Compare different chunk sizes for retrieval quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Details Test\n",
    "Compare chunk sizes 100 vs 1024 for finding specific mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¬ Testing chunk size impact on fine details:\")\n",
    "print(\"\\nQuery: 'Describe the relationship between Alexander and Diogenes.'\")\n",
    "print(\"\\nWith chunk size = 100:\")\n",
    "# Results would show better detail retrieval\n",
    "print(\"Alexander admired Diogenes' conduct. Diogenes requested that Alexander and his attendants move out of the sunlight.\")\n",
    "\n",
    "print(\"\\nWith chunk size = 1024:\")\n",
    "print(\"The provided text does not contain information about Diogenes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Query Test\n",
    "Test multi-faceted questions requiring synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¬ Testing complex query handling:\")\n",
    "print(\"Query: 'What was Alexander's strategy against Darius III?'\")\n",
    "print(\"\\nChunk size = 100: Focuses on tactical details\")\n",
    "print(\"Alexander led a quick charge with a wedge formation of Companion cavalry...\")\n",
    "\n",
    "print(\"\\nChunk size = 1024: Broader strategic overview\")\n",
    "print(\"Alexander was advised to advance against Darius and the Persians without delay...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hypothetical Document Embedding (HyDE)\n",
    "Generate hypothetical answers to improve retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "def create_hypothetical_answer(question):\n",
    "    \"\"\"\n",
    "    Generate a hypothetical answer to improve retrieval.\n",
    "    \n",
    "    Uses LLM to create an educated guess about the answer,\n",
    "    which is then used for semantic search.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        ChatMessage(\n",
    "            role=\"system\",\n",
    "            content=\"Answer the following question in 2-3 sentences. \"\n",
    "                   \"If you don't know the answer, make an educated guess.\"\n",
    "        ),\n",
    "        ChatMessage(role=\"user\", content=question)\n",
    "    ]\n",
    "    answer = str(llm.chat(messages))\n",
    "    return answer\n",
    "\n",
    "def hyde_rag(question):\n",
    "    \"\"\"\n",
    "    HyDE-enhanced RAG function.\n",
    "    \n",
    "    1. Generate hypothetical answer\n",
    "    2. Use it for semantic search\n",
    "    3. Return results\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ¤” Original question: {question}\")\n",
    "    answer = create_hypothetical_answer(question)\n",
    "    print(f\"ðŸ’­ Hypothetical answer: {answer}\\n\")\n",
    "    \n",
    "    return semantic_rag(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE Test 1: Complex Strategy Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§ª HyDE Test: Alexander's strategy against Darius III\")\n",
    "hyde_response = hyde_rag(\"What was Alexander's strategy against Darius III?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE Test 2: Relationship Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ§ª HyDE Test: Alexander and Diogenes relationship\")\n",
    "hyde_response = hyde_rag(\"Describe the relationship between Alexander and Diogenes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query Expansion\n",
    "Add context and synonyms to improve retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context_to_query(question):\n",
    "    \"\"\"\n",
    "    Expand query with historical context and synonyms.\n",
    "    \n",
    "    Adds 2nd century names, geographical context, and clarifies terms\n",
    "    for better matching with historical texts.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        ChatMessage(\n",
    "            role=\"system\",\n",
    "            content=\"\"\"The following question is about topics discussed in a 2nd century book about Alexander the Great.\n",
    "            Clarify the question posed in the following ways:\n",
    "            * Expand to include 2nd century names. For example, a question about Iranians should include answers about Parthians, Persians, Medes, Bactrians, etc.\n",
    "            * Provide context on terms. For example, that Ammonites came from Jordan or that Philip was the father of Alexander.\n",
    "            Provide only the clarified question without any preamble or instructions.\"\"\"\n",
    "        ),\n",
    "        ChatMessage(role=\"user\", content=question)\n",
    "    ]\n",
    "    expanded_question = str(llm.chat(messages))\n",
    "    return expanded_question\n",
    "\n",
    "def query_expansion_rag(question):\n",
    "    \"\"\"\n",
    "    Query expansion-enhanced RAG function.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Original question: {question}\")\n",
    "    expanded_question = add_context_to_query(question)\n",
    "    print(f\"ðŸ“ Expanded question: {expanded_question}\\n\")\n",
    "    \n",
    "    return semantic_rag(expanded_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Expansion Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Query Expansion Test: Persian military tactics\")\n",
    "qe_response = query_expansion_rag(\"How did the Persian king fight the Greeks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hybrid Search\n",
    "Combine semantic and keyword search approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_hybrid_search_alpha_values():\n",
    "    \"\"\"\n",
    "    Test different alpha values for hybrid search.\n",
    "    \n",
    "    Alpha controls the balance between:\n",
    "    - alpha=0: Pure keyword search\n",
    "    - alpha=1: Pure semantic search\n",
    "    - alpha=0.5: Equal weighting\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”¬ Testing hybrid search with different alpha values...\")\n",
    "    \n",
    "    # Note: In-memory vector store doesn't support hybrid search\n",
    "    # This would work with Weaviate, Postgres, Pinecone, etc.\n",
    "    \n",
    "    for alpha in np.arange(0, 1.1, 0.5):\n",
    "        print(f\"\\nâš¡ Alpha = {alpha}\")\n",
    "        try:\n",
    "            query_engine = RetrieverQueryEngine.from_args(\n",
    "                retriever=index.as_retriever(similarity_top_k=TOP_K),\n",
    "                llm=llm,\n",
    "                vector_store_query_mode=\"hybrid\",\n",
    "                alpha=alpha,\n",
    "                disable_cache=True,\n",
    "            )\n",
    "            \n",
    "            # This would work with supported vector stores\n",
    "            print(\"   Hybrid search configured (requires supported vector store)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Hybrid search not supported with current vector store\")\n",
    "            print(f\"   Error: {str(e)[:100]}...\")\n",
    "\n",
    "# Test hybrid search configuration\n",
    "test_hybrid_search_alpha_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Query Examples\n",
    "Demonstrate combined techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-technique Query 1: Military Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Advanced Query: Comprehensive military strategy analysis\")\n",
    "advanced_query = \"What were Alexander's key military innovations and how did they contribute to his victories against Persian forces?\"\n",
    "\n",
    "# Try with HyDE first\n",
    "print(\"Using HyDE approach:\")\n",
    "hyde_result = hyde_rag(advanced_query)\n",
    "\n",
    "# Then try with query expansion\n",
    "print(\"\\nUsing Query Expansion approach:\")\n",
    "qe_result = query_expansion_rag(advanced_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-technique Query 2: Cultural Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸš€ Advanced Query: Alexander's cultural legacy\")\n",
    "cultural_query = \"How did Alexander's conquests influence the spread of Greek culture and language across the ancient world?\"\n",
    "\n",
    "print(\"Comparing approaches:\")\n",
    "print(\"\\n1. Basic Semantic RAG:\")\n",
    "basic_result = semantic_rag(cultural_query)\n",
    "\n",
    "print(\"\\n2. HyDE Enhanced:\")\n",
    "hyde_result = hyde_rag(cultural_query)\n",
    "\n",
    "print(\"\\n3. Query Expansion:\")\n",
    "qe_result = query_expansion_rag(cultural_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Analysis\n",
    "Compare retrieval quality across different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_retrieval_approaches():\n",
    "    \"\"\"\n",
    "    Analyze and compare different retrieval approaches.\n",
    "    \"\"\"\n",
    "    test_questions = [\n",
    "        \"What was Alexander's relationship with his generals?\",\n",
    "        \"How did Alexander handle resistance from conquered cities?\",\n",
    "        \"What role did religion play in Alexander's campaigns?\",\n",
    "        \"Describe Alexander's administrative policies in conquered territories.\"\n",
    "    ]\n",
    "    \n",
    "    approaches = {\n",
    "        \"Basic Semantic\": semantic_rag,\n",
    "        \"HyDE Enhanced\": hyde_rag,\n",
    "        \"Query Expansion\": query_expansion_rag\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ“Š Retrieval Approach Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\nðŸ“ Question: {question}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for approach_name, approach_func in approaches.items():\n",
    "            print(f\"\\n{approach_name}:\")\n",
    "            try:\n",
    "                result = approach_func(question)\n",
    "                # Analyze result quality (simplified)\n",
    "                answer_length = len(result['answer'])\n",
    "                source_count = len(result['source_nodes'])\n",
    "                avg_score = sum(node.score for node in result['source_nodes']) / source_count if source_count > 0 else 0\n",
    "                \n",
    "                print(f\"  Answer length: {answer_length} chars\")\n",
    "                print(f\"  Sources found: {source_count}\")\n",
    "                print(f\"  Avg relevance: {avg_score:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {str(e)[:50]}...\")\n",
    "\n",
    "# Run analysis\n",
    "analyze_retrieval_approaches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Best Practices and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“‹ Best Practices for Index-aware Retrieval:\")\n",
    "print(\"\\n1. **Chunk Size Selection**:\")\n",
    "print(\"   â€¢ Small chunks (100-300): Better for specific details\")\n",
    "print(\"   â€¢ Large chunks (500-1500): Better for context and coherence\")\n",
    "print(\"   â€¢ Consider hybrid approach with multiple chunk sizes\")\n",
    "\n",
    "print(\"\\n2. **HyDE Usage**:\")\n",
    "print(\"   â€¢ Effective for complex, multi-faceted questions\")\n",
    "print(\"   â€¢ Helps find relevant passages that don't contain query terms\")\n",
    "print(\"   â€¢ Use when basic semantic search fails\")\n",
    "\n",
    "print(\"\\n3. **Query Expansion**:\")\n",
    "print(\"   â€¢ Essential for historical/technical texts\")\n",
    "print(\"   â€¢ Add synonyms, historical names, context\")\n",
    "print(\"   â€¢ Particularly useful for domain-specific queries\")\n",
    "\n",
    "print(\"\\n4. **Hybrid Search**:\")\n",
    "print(\"   â€¢ Combines best of keyword and semantic search\")\n",
    "print(\"   â€¢ Requires supported vector database\")\n",
    "print(\"   â€¢ Tune alpha parameter based on your data\")\n",
    "\n",
    "print(\"\\n5. **General Tips**:\")\n",
    "print(\"   â€¢ Test different approaches on your specific data\")\n",
    "print(\"   â€¢ Monitor retrieval quality and adjust parameters\")\n",
    "print(\"   â€¢ Consider ensemble approaches combining multiple methods\")\n",
    "print(\"   â€¢ Always validate results for factual accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Next Steps and Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Enhancements:\n",
    "\n",
    "1. **Graph RAG Implementation**:\n",
    "   - Build knowledge graphs from text\n",
    "   - Use entity relationships for retrieval\n",
    "   - Implement graph-based reasoning\n",
    "\n",
    "2. **Multi-modal Retrieval**:\n",
    "   - Add image and audio processing\n",
    "   - Cross-modal search capabilities\n",
    "   - Handle different content types\n",
    "\n",
    "3. **Personalization**:\n",
    "   - User preference learning\n",
    "   - Contextual retrieval based on history\n",
    "   - Adaptive ranking algorithms\n",
    "\n",
    "4. **Real-time Updates**:\n",
    "   - Incremental indexing\n",
    "   - Streaming data processing\n",
    "   - Dynamic knowledge base updates\n",
    "\n",
    "5. **Evaluation Framework**:\n",
    "   - Automated retrieval quality metrics\n",
    "   - A/B testing for different approaches\n",
    "   - User feedback integration\n",
    "\n",
    "### Advanced Techniques to Explore:\n",
    "\n",
    "- **Retrieval with Reinforcement Learning**: Train models to optimize retrieval quality\n",
    "- **Multi-hop Reasoning**: Connect information across multiple documents\n",
    "- **Temporal-aware Retrieval**: Consider time-based relationships\n",
    "- **Cross-lingual Retrieval**: Handle multiple languages\n",
    "- **Federated Retrieval**: Search across multiple knowledge bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ¯ Ready to implement these advanced retrieval techniques in your projects!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
