{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Experimentation Notebook\n",
    "## Harry Potter Book Analysis with Multi-Strategy Retrieval\n",
    "\n",
    "This notebook demonstrates an advanced RAG system that analyzes Harry Potter books using multiple retrieval strategies, question rewriting, chain-of-thought reasoning, and a sophisticated plan-and-execute agent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Environment Setup\n",
    "\n",
    "Load environment variables and set up API keys for OpenAI and Groq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment and API Setup ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (e.g., API keys)\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Set API Keys for OpenAI and Groq\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "# Optional: Set environment variable for debugging (increases timeout)\n",
    "os.environ[\"PYDEVD_WARN_EVALUATION_TIMEOUT\"] = \"100000\"\n",
    "\n",
    "print(\"‚úÖ Environment variables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üìñ Data Loading & Preprocessing\n",
    "\n",
    "### 1.1 Load Harry Potter PDF and Split into Chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries for Document Loading ---\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from helper_functions import split_into_chapters, replace_t_with_space\n",
    "\n",
    "# Define the path to the Harry Potter PDF file\n",
    "hp_pdf_path = \"Harry_Potter_Book_1_The_Sorcerers_Stone.pdf\"\n",
    "\n",
    "# Split the PDF into chapters and preprocess the text\n",
    "print(\"üìö Loading and splitting PDF into chapters...\")\n",
    "chapters = split_into_chapters(hp_pdf_path)\n",
    "chapters = replace_t_with_space(chapters)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(chapters)} chapters from the book.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Extract Book Quotes as Separate Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract Book Quotes ---\n",
    "from helper_functions import extract_book_quotes_as_documents\n",
    "\n",
    "# Load the PDF and extract quotes\n",
    "print(\"üìñ Extracting book quotes...\")\n",
    "loader = PyPDFLoader(hp_pdf_path)\n",
    "document = loader.load()\n",
    "document_cleaned = replace_t_with_space(document)\n",
    "book_quotes_list = extract_book_quotes_as_documents(document_cleaned)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(book_quotes_list)} quotes from the book.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üß† Chapter Summarization with LLMs\n",
    "\n",
    "Generate summaries for each chapter using GPT-3.5-turbo to enable high-level retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summarization Setup ---\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from time import monotonic\n",
    "\n",
    "# Define the prompt template for summarization\n",
    "summarization_prompt = PromptTemplate(\n",
    "    template=\"\"\"Write an extensive summary of the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "SUMMARY:\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "def create_chapter_summary(chapter):\n",
    "    \"\"\"Creates a summary of a chapter using GPT-3.5-turbo.\"\"\"\n",
    "    chapter_txt = chapter.page_content\n",
    "    \n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-0125\")\n",
    "    \n",
    "    # Load the summarization chain\n",
    "    chain = load_summarize_chain(\n",
    "        llm,\n",
    "        chain_type=\"stuff\",\n",
    "        prompt=summarization_prompt,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Generate the summary\n",
    "    start_time = monotonic()\n",
    "    doc_chapter = Document(page_content=chapter_txt)\n",
    "    summary_result = chain.invoke([doc_chapter])\n",
    "    \n",
    "    print(f\"‚ú® Summarized in {monotonic() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Clean and return the summary\n",
    "    from helper_functions import replace_double_lines_with_one_line\n",
    "    summary_text = replace_double_lines_with_one_line(summary_result[\"output_text\"])\n",
    "    return Document(page_content=summary_text, metadata=chapter.metadata)\n",
    "\n",
    "# Generate summaries for all chapters\n",
    "print(\"üìù Generating summaries for each chapter...\")\n",
    "chapter_summaries = []\n",
    "\n",
    "for i, chapter in enumerate(chapters):\n",
    "    print(f\"\\n--- Processing Chapter {i+1} ---\")\n",
    "    summary = create_chapter_summary(chapter)\n",
    "    chapter_summaries.append(summary)\n",
    "    print(f\"Chapter {i+1} summary length: {len(summary.page_content)} characters\")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(chapter_summaries)} chapter summaries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîç Vector Store Creation\n",
    "\n",
    "Create three separate vector stores: book chunks, chapter summaries, and book quotes. Each serves a different retrieval purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vector Store Imports ---\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def encode_book(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Encodes a PDF book into a FAISS vector store.\"\"\"\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    return FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "# Create embeddings instance\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Check if vector stores exist on disk\n",
    "vector_store_paths = {\n",
    "    \"chunks\": \"chunks_vector_store\",\n",
    "    \"summaries\": \"chapter_summaries_vector_store\", \n",
    "    \"quotes\": \"book_quotes_vectorstore\"\n",
    "}\n",
    "\n",
    "if all(os.path.exists(path) for path in vector_store_paths.values()):\n",
    "    print(\"üìÇ Loading existing vector stores...\")\n",
    "    chunks_vector_store = FAISS.load_local(vector_store_paths[\"chunks\"], embeddings, allow_dangerous_deserialization=True)\n",
    "    chapter_summaries_vector_store = FAISS.load_local(vector_store_paths[\"summaries\"], embeddings, allow_dangerous_deserialization=True)\n",
    "    book_quotes_vectorstore = FAISS.load_local(vector_store_paths[\"quotes\"], embeddings, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"üîÑ Creating new vector stores...\")\n",
    "    # Encode and save the vector stores\n",
    "    chunks_vector_store = encode_book(hp_pdf_path, chunk_size=1000, chunk_overlap=200)\n",
    "    chapter_summaries_vector_store = FAISS.from_documents(chapter_summaries, embeddings)\n",
    "    book_quotes_vectorstore = FAISS.from_documents(book_quotes_list, embeddings)\n",
    "    \n",
    "    # Save to disk for future use\n",
    "    chunks_vector_store.save_local(vector_store_paths[\"chunks\"])\n",
    "    chapter_summaries_vector_store.save_local(vector_store_paths[\"summaries\"])\n",
    "    book_quotes_vectorstore.save_local(vector_store_paths[\"quotes\"])\n",
    "    \n",
    "print(\"‚úÖ Vector stores ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Retrievers from Vector Stores ---\n",
    "# Different 'k' values for different retrieval strategies\n",
    "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "chapter_summaries_query_retriever = chapter_summaries_vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "book_quotes_query_retriever = book_quotes_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(\"üîç Retrievers created with different configurations:\")\n",
    "print(\"- Book chunks: k=3 (detailed information)\")\n",
    "print(\"- Chapter summaries: k=2 (high-level overview)\")\n",
    "print(\"- Book quotes: k=5 (specific evidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üéØ Basic RAG Pipeline\n",
    "\n",
    "Test a simple RAG query to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic RAG Function ---\n",
    "def basic_rag_query(question):\n",
    "    \"\"\"Simple RAG query using all retrievers.\"\"\"\n",
    "    # Retrieve from all sources\n",
    "    docs = chunks_query_retriever.get_relevant_documents(question)\n",
    "    docs_summaries = chapter_summaries_query_retriever.get_relevant_documents(question)\n",
    "    docs_quotes = book_quotes_query_retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Combine all contexts\n",
    "    context = \" \".join([doc.page_content for doc in docs + docs_summaries + docs_quotes])\n",
    "    \n",
    "    # Generate answer using LLM\n",
    "    from helper_functions import escape_quotes\n",
    "    context = escape_quotes(context)\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nProvide a concise answer:\"\n",
    "    \n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "# Test basic RAG\n",
    "print(\"üß™ Testing Basic RAG Pipeline:\")\n",
    "test_question = \"Who is Fluffy?\"\n",
    "result = basic_rag_query(test_question)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîÑ Advanced RAG with LangGraph\n",
    "\n",
    "### 5.1 Question Rewriting for Better Retrieval\n",
    "\n",
    "Rewrite questions to improve vector store retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Question Rewriting Setup ---\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class RewriteQuestion(BaseModel):\n",
    "    rewritten_question: str = Field(description=\"Improved question for retrieval\")\n",
    "    explanation: str = Field(description=\"Explanation of changes\")\n",
    "\n",
    "rewrite_llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    groq_api_key=groq_api_key,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "rewrite_chain = PromptTemplate(\n",
    "    template=\"\"\"You are a question re-writer that converts an input question to a better version optimized for vectorstore retrieval.\n",
    "Analyze the input question {question} and try to reason about the underlying semantic intent / meaning.\n",
    "\n",
    "{format_instructions}\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": JsonOutputParser(pydantic_object=RewriteQuestion).get_format_instructions()}\n",
    ") | rewrite_llm | JsonOutputParser(pydantic_object=RewriteQuestion)\n",
    "\n",
    "# Test question rewriting\n",
    "print(\"‚úçÔ∏è Testing Question Rewriting:\")\n",
    "test_question = \"stuff about the three-headed dog\"\n",
    "rewritten = rewrite_chain.invoke({\"question\": test_question})\n",
    "print(f\"Original: {test_question}\")\n",
    "print(f\"Rewritten: {rewritten['rewritten_question']}\")\n",
    "print(f\"Explanation: {rewritten['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Chain-of-Thought Answering\n",
    "\n",
    "Use step-by-step reasoning to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chain-of-Thought Setup ---\n",
    "class QuestionAnswerFromContext(BaseModel):\n",
    "    answer_based_on_content: str = Field(description=\"Answer based on context\")\n",
    "\n",
    "cot_chain = PromptTemplate(\n",
    "    template=\"\"\"Answer this question using chain-of-thought reasoning.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Think step by step and provide your reasoning before the final answer.\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ") | ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=2000) | JsonOutputParser(pydantic_object=QuestionAnswerFromContext)\n",
    "\n",
    "# Test CoT answering\n",
    "print(\"üß† Testing Chain-of-Thought Answering:\")\n",
    "test_context = \"Harry Potter is a young wizard. He discovers he is famous in the wizarding world for surviving an attack by Voldemort as a baby.\"\n",
    "test_q = \"Why is Harry Potter famous?\"\n",
    "\n",
    "result = cot_chain.invoke({\"context\": test_context, \"question\": test_q})\n",
    "print(f\"Question: {test_q}\")\n",
    "print(f\"Answer: {result['answer_based_on_content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üó∫Ô∏è Sophisticated Pipeline with Plan-and-Execute\n",
    "\n",
    "### 6.1 Question Anonymization\n",
    "\n",
    "Replace named entities with variables to create unbiased plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Question Anonymization Setup ---\n",
    "class AnonymizeQuestion(BaseModel):\n",
    "    anonymized_question: str = Field(description=\"Question with entities replaced by variables\")\n",
    "    mapping: dict = Field(description=\"Mapping of variables to original entities\")\n",
    "    explanation: str = Field(description=\"Explanation of the process\")\n",
    "\n",
    "anonymize_chain = PromptTemplate(\n",
    "    template=\"\"\"You are a question anonymizer. Replace all name entities in the question with variables.\n",
    "\n",
    "Example: \"who is harry potter?\" ‚Üí \"who is X?\" with mapping {\"X\": \"harry potter\"}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "{format_instructions}\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": JsonOutputParser(pydantic_object=AnonymizeQuestion).get_format_instructions()}\n",
    ") | ChatOpenAI(temperature=0, model_name=\"gpt-4o\") | JsonOutputParser(pydantic_object=AnonymizeQuestion)\n",
    "\n",
    "# Test anonymization\n",
    "print(\"üé≠ Testing Question Anonymization:\")\n",
    "test_q = \"how did harry beat quirrell?\"\n",
    "result = anonymize_chain.invoke({\"question\": test_q})\n",
    "print(f\"Original: {test_q}\")\n",
    "print(f\"Anonymized: {result['anonymized_question']}\")\n",
    "print(f\"Mapping: {result['mapping']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Multi-Step Planning\n",
    "\n",
    "Create step-by-step plans to answer complex questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Planning Setup ---\n",
    "from typing import List\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    steps: List[str] = Field(description=\"Ordered list of steps to answer the question\")\n",
    "\n",
    "planner_chain = PromptTemplate(\n",
    "    template=\"\"\"Create a step-by-step plan to answer this question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Rules:\n",
    "1. Each step should be executable by retrieval or answering\n",
    "2. Include all necessary information\n",
    "3. Don't skip steps\"\"\",\n",
    "    input_variables=[\"question\"]\n",
    ") | ChatOpenAI(temperature=0, model_name=\"gpt-4o\") | JsonOutputParser(pydantic_object=Plan)\n",
    "\n",
    "# Test planning\n",
    "print(\"üìã Testing Multi-Step Planning:\")\n",
    "complex_question = \"What class does the professor who helped the villain teach?\"\n",
    "plan = planner_chain.invoke({\"question\": complex_question})\n",
    "print(f\"Question: {complex_question}\")\n",
    "print(\"Plan Steps:\")\n",
    "for i, step in enumerate(plan['steps'], 1):\n",
    "    print(f\"{i}. {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Plan Refinement\n",
    "\n",
    "Refine plans to be more specific and executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plan Refinement Setup ---\n",
    "class DeAnonymizePlan(BaseModel):\n",
    "    plan: List[str] = Field(description=\"Plan with variables replaced by original entities\")\n",
    "\n",
    "deanonymize_chain = PromptTemplate(\n",
    "    template=\"\"\"Replace variables in this plan with the mapped words.\n",
    "\n",
    "Plan: {plan}\n",
    "Mapping: {mapping}\n",
    "\n",
    "Return the updated plan list.\"\"\",\n",
    "    input_variables=[\"plan\", \"mapping\"]\n",
    ") | ChatOpenAI(temperature=0, model_name=\"gpt-4o\") | JsonOutputParser(pydantic_object=DeAnonymizePlan)\n",
    "\n",
    "# Test full pipeline: anonymize ‚Üí plan ‚Üí deanonymize\n",
    "print(\"üîó Testing Full Planning Pipeline:\")\n",
    "question = \"how did the main character beat the villain?\"\n",
    "\n",
    "# Step 1: Anonymize\n",
    "anon_result = anonymize_chain.invoke({\"question\": question})\n",
    "print(f\"Anonymized: {anon_result['anonymized_question']}\")\n",
    "\n",
    "# Step 2: Plan\n",
    "plan_result = planner_chain.invoke({\"question\": anon_result['anonymized_question']})\n",
    "print(f\"Plan for anonymized question: {plan_result['steps']}\")\n",
    "\n",
    "# Step 3: De-anonymize\n",
    "deanon_result = deanonymize_chain.invoke({\n",
    "    \"plan\": plan_result['steps'],\n",
    "    \"mapping\": anon_result['mapping']\n",
    "})\n",
    "print(f\"Final plan: {deanon_result['plan']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üß™ Experimental Section: Testing Different Strategies\n",
    "\n",
    "Compare different retrieval strategies to see which works best for different question types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrieval Strategy Comparison ---\n",
    "def compare_retrieval_strategies(question):\n",
    "    \"\"\"Compare different retrieval approaches for the same question.\"\"\"\n",
    "    strategies = {\n",
    "        \"Basic Chunks\": chunks_query_retriever,\n",
    "        \"Chapter Summaries\": chapter_summaries_query_retriever,\n",
    "        \"Book Quotes\": book_quotes_query_retriever,\n",
    "        \"Combined\": \"all\"  # Special flag to combine all\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for strategy_name, retriever in strategies.items():\n",
    "        print(f\"\\nüéØ Testing {strategy_name}...\")\n",
    "        \n",
    "        if strategy_name == \"Combined\":\n",
    "            # Combine all contexts\n",
    "            docs = chunks_query_retriever.get_relevant_documents(question)\n",
    "            docs_summaries = chapter_summaries_query_retriever.get_relevant_documents(question)\n",
    "            docs_quotes = book_quotes_query_retriever.get_relevant_documents(question)\n",
    "            context = \" \".join([doc.page_content for doc in docs + docs_summaries + docs_quotes])\n",
    "        else:\n",
    "            docs = retriever.get_relevant_documents(question)\n",
    "            context = \" \".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        print(f\"Retrieved {len(context)} characters of context\")\n",
    "        \n",
    "        # Generate answer\n",
    "        llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "        prompt = f\"Context: {context[:1000]}...\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        answer = llm.invoke(prompt).content\n",
    "        \n",
    "        results[strategy_name] = answer\n",
    "        print(f\"Answer: {answer[:100]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "test_question = \"What spell does Hermione use to fix Harry's glasses?\"\n",
    "print(f\"üß™ Comparing strategies for: {test_question}\")\n",
    "comparison_results = compare_retrieval_strategies(test_question)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL COMPARISON:\")\n",
    "print(\"=\"*60)\n",
    "for strategy, answer in comparison_results.items():\n",
    "    print(f\"\\n{strategy}:\")\n",
    "    print(f\"  ‚Üí {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Question Complexity Levels ---\n",
    "complex_questions = [\n",
    "    \"Simple: Who is Harry Potter?\",\n",
    "    \"Moderate: What house is Harry in and why?\",\n",
    "    \"Complex: How does Harry's relationship with Dumbledore evolve?\",\n",
    "    \"Reasoning: What does the Mirror of Erised reveal about human nature?\"\n",
    "]\n",
    "\n",
    "def test_complexity_levels():\n",
    "    \"\"\"Test how the system handles different question complexities.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for question in complex_questions:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing: {question}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Extract actual question (remove prefix)\n",
    "        actual_question = question.split(\": \", 1)[1]\n",
    "        \n",
    "        # Test basic RAG\n",
    "        print(\"\\n[Basic RAG]\")\n",
    "        basic_answer = basic_rag_query(actual_question)\n",
    "        print(f\"Answer: {basic_answer}\")\n",
    "        \n",
    "        # Test with question rewriting\n",
    "        print(\"\\n[With Question Rewriting]\")\n",
    "        rewritten = rewrite_chain.invoke({\"question\": actual_question})\n",
    "        rewritten_answer = basic_rag_query(rewritten['rewritten_question'])\n",
    "        print(f\"Rewritten Q: {rewritten['rewritten_question']}\")\n",
    "        print(f\"Answer: {rewritten_answer}\")\n",
    "        \n",
    "        results[question] = {\n",
    "            \"basic\": basic_answer,\n",
    "            \"rewritten\": rewritten_answer\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run complexity test\n",
    "print(\"üß† Testing Different Question Complexity Levels\")\n",
    "complexity_results = test_complexity_levels()\n",
    "\n",
    "# Save results for analysis\n",
    "import json\n",
    "with open(\"complexity_test_results.json\", \"w\") as f:\n",
    "    json.dump(complexity_results, f, indent=2)\n",
    "print(\"\\nüíæ Results saved to complexity_test_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üìä Evaluation with Ragas\n",
    "\n",
    "Use Ragas framework to quantitatively evaluate our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ragas Evaluation Setup ---\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "# Evaluation questions and ground truth answers\n",
    "eval_questions = [\n",
    "    \"What is the name of the three-headed dog?\",\n",
    "    \"Who gave Harry his first broomstick?\",\n",
    "    \"Which house did the Sorting Hat initially consider for Harry?\",\n",
    "    \"What is the name of Harry's owl?\",\n",
    "    \"How did Harry and his friends get past Fluffy?\"\n",
    "]\n",
    "\n",
    "ground_truth_answers = [\n",
    "    \"Fluffy\",\n",
    "    \"Professor McGonagall\",\n",
    "    \"Slytherin\",\n",
    "    \"Hedwig\",\n",
    "    \"They played music to put Flummy to sleep\"\n",
    "]\n",
    "\n",
    "print(\"üéØ Starting Ragas Evaluation...\")\n",
    "print(f\"Evaluating {len(eval_questions)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate Answers and Collect Contexts ---\n",
    "generated_answers = []\n",
    "retrieved_contexts = []\n",
    "\n",
    "print(\"\\nü§ñ Generating answers for evaluation questions...\")\n",
    "for i, question in enumerate(eval_questions):\n",
    "    print(f\"\\n[{i+1}/{len(eval_questions)}] Q: {question}\")\n",
    "    \n",
    "    # Generate answer using basic RAG\n",
    "    answer = basic_rag_query(question)\n",
    "    generated_answers.append(answer)\n",
    "    print(f\"A: {answer}\")\n",
    "    \n",
    "    # Collect contexts used\n",
    "    docs = chunks_query_retriever.get_relevant_documents(question)\n",
    "    contexts = [doc.page_content for doc in docs]\n",
    "    retrieved_contexts.append(contexts)\n",
    "    print(f\"Retrieved {len(contexts)} context chunks\")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(generated_answers)} answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Ragas Evaluation ---\n",
    "# Prepare data for Ragas\n",
    "data_samples = {\n",
    "    'question': eval_questions,\n",
    "    'answer': generated_answers,\n",
    "    'contexts': retrieved_contexts,\n",
    "    'ground_truth': ground_truth_answers\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "print(\"üìä Running Ragas evaluation...\")\n",
    "# Initialize LLM for Ragas evaluation\n",
    "eval_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[\n",
    "        answer_correctness,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        answer_similarity\n",
    "    ],\n",
    "    llm=eval_llm\n",
    ")\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "results_df = results.to_pandas()\n",
    "print(\"\\nüìà Evaluation Results:\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Calculate and display mean scores\n",
    "print(f\"\\nüìä Mean Scores:\")\n",
    "for metric in ['answer_correctness', 'faithfulness', 'answer_relevancy', 'context_recall', 'answer_similarity']:\n",
    "    mean_score = results_df[metric].mean()\n",
    "    print(f\"  {metric}: {mean_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üî¨ Advanced Experiments\n",
    "\n",
    "### 9.1 Test Hallucination Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hallucination Detection Test ---\n",
    "def test_hallucination_scenarios():\n",
    "    \"\"\"Test the system's ability to detect and prevent hallucinations.\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"question\": \"What is Harry's favorite color?\",\n",
    "            \"context\": \"Harry Potter is a wizard who attends Hogwarts School of Witchcraft and Wizardry.\",\n",
    "            \"type\": \"insufficient_context\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Who is Hagrid?\",\n",
    "            \"context\": \"Rubeus Hagrid is the gamekeeper at Hogwarts and a loyal friend to Harry.\",\n",
    "            \"type\": \"sufficient_context\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is Voldemort's favorite ice cream flavor?\",\n",
    "            \"context\": \"Lord Voldemort is the main antagonist in the Harry Potter series.\",\n",
    "            \"type\": \"not_in_book\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ Testing Hallucination Detection Scenarios\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test['type'].replace('_', ' ').title()}\")\n",
    "        print(f\"Q: {test['question']}\")\n",
    "        print(f\"Context: {test['context']}\")\n",
    "        \n",
    "        # Try to answer and see if it hallucinates\n",
    "        try:\n",
    "            answer = basic_rag_query(test['question'])\n",
    "            print(f\"Generated Answer: {answer}\")\n",
    "            \n",
    "            # Check if answer is grounded in context\n",
    "            if test['type'] == 'insufficient_context' or test['type'] == 'not_in_book':\n",
    "                print(\"‚ö†Ô∏è  Should ideally respond with 'I don't have enough information'\")\n",
    "            else:\n",
    "                print(\"‚úÖ Should provide a factual answer\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Run hallucination tests\n",
    "test_hallucination_scenarios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Chain-of-Thought Analysis\n",
    "\n",
    "Deep dive into the reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chain-of-Thought Analysis ---\n",
    "def analyze_cot_detailed(question, context):\n",
    "    \"\"\"Get detailed chain-of-thought reasoning for a question.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze this step by step:\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Provide:\n",
    "1. Key facts from context\n",
    "2. Missing information (if any)\n",
    "3. Logical connections\n",
    "4. Final answer with reasoning\"\"\"\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# Test CoT analysis on a complex question\n",
    "cot_test_question = \"Why does Harry survive Voldemort's curse?\"\n",
    "cot_test_context = \"Harry's mother's love protected him, creating a powerful magical shield. This ancient magic is stronger than any dark curse.\"\n",
    "\n",
    "print(f\"üîç Chain-of-Thought Analysis:\")\n",
    "print(f\"Question: {cot_test_question}\")\n",
    "print(f\"\\nContext: {cot_test_context}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "detailed_analysis = analyze_cot_detailed(cot_test_question, cot_test_context)\n",
    "print(detailed_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üìà Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization of Ragas Results ---\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_ragas_results(results_df):\n",
    "    \"\"\"Create visual summary of Ragas evaluation results.\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('RAGAS Evaluation Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    x_pos = np.arange(len(results_df))\n",
    "    \n",
    "    # Answer Correctness\n",
    "    ax1.bar(x_pos, results_df['answer_correctness'], color='skyblue', alpha=0.8)\n",
    "    ax1.set_title('Answer Correctness', fontweight='bold')\n",
    "    ax1.set_ylabel('Score (0-1)')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Faithfulness\n",
    "    ax2.bar(x_pos, results_df['faithfulness'], color='lightcoral', alpha=0.8)\n",
    "    ax2.set_title('Faithfulness', fontweight='bold')\n",
    "    ax2.set_ylabel('Score (0-1)')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Answer Relevancy\n",
    "    ax3.bar(x_pos, results_df['answer_relevancy'], color='lightgreen', alpha=0.8)\n",
    "    ax3.set_title('Answer Relevancy', fontweight='bold')\n",
    "    ax3.set_ylabel('Score (0-1)')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    \n",
    "    # Context Recall\n",
    "    ax4.bar(x_pos, results_df['context_recall'], color='gold', alpha=0.8)\n",
    "    ax4.set_title('Context Recall', fontweight='bold')\n",
    "    ax4.set_ylabel('Score (0-1)')\n",
    "    ax4.set_ylim(0, 1)\n",
    "    \n",
    "    # Set x-axis labels for all subplots\n",
    "    for ax in [ax1, ax2, ax3, ax4]:\n",
    "        ax.set_xlabel('Question Index')\n",
    "        ax.set_xticks(x_pos)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nüìä Summary Statistics:\")\n",
    "    print(results_df.describe().round(3))\n",
    "\n",
    "# Visualize the results\n",
    "if 'results_df' in globals():\n",
    "    visualize_ragas_results(results_df)\n",
    "else:\n",
    "    print(\"‚ùå Run the Ragas evaluation first to generate results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Performance Comparison Table ---\n",
    "def create_performance_summary():\n",
    "    \"\"\"Create a summary table of different RAG approaches tested.\"\"\"\n",
    "    \n",
    "    # This would normally come from systematic testing\n",
    "    # For now, create a sample summary\n",
    "    \n",
    "    summary_data = {\n",
    "        \"Approach\": [\n",
    "            \"Basic RAG (Chunks Only)\",\n",
    "            \"Basic RAG (All Sources)\",\n",
    "            \"RAG + Question Rewriting\",\n",
    "            \"RAG + CoT Reasoning\",\n",
    "            \"Plan-and-Execute Agent\"\n",
    "        ],\n",
    "        \"Answer Quality\": [\"‚≠ê‚≠ê\", \"‚≠ê‚≠ê‚≠ê\", \"‚≠ê‚≠ê‚≠ê\", \"‚≠ê‚≠ê‚≠ê‚≠ê\", \"‚≠ê‚≠ê‚≠ê‚≠ê\"],\n",
    "        \"Speed\": [\"‚ö°‚ö°‚ö°‚ö°\", \"‚ö°‚ö°‚ö°\", \"‚ö°‚ö°\", \"‚ö°\", \"‚ö°\"],\n",
    "        \"Complexity\": [\"Low\", \"Low\", \"Medium\", \"Medium\", \"High\"],\n",
    "        \"Best For\": [\n",
    "            \"Simple fact lookup\",\n",
    "            \"General questions\",\n",
    "            \"Poorly formulated questions\",\n",
    "            \"Reasoning questions\",\n",
    "            \"Multi-hop questions\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"üìà RAG Approaches Comparison:\")\n",
    "    print(\"=\"*80)\n",
    "    return summary_df\n",
    "\n",
    "try:\n",
    "    comparison_table = create_performance_summary()\n",
    "    print(comparison_table.to_string(index=False))\n",
    "except:\n",
    "    print(\"üìä Performance comparison table requires pandas\")\n",
    "    print(\"Install with: pip install pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. üéØ Key Takeaways & Experimentation Ideas\n",
    "\n",
    "### 11.1 What Works Well\n",
    "- ‚úÖ **Multiple retrieval strategies** provide comprehensive coverage\n",
    "- ‚úÖ **Chapter summaries** help with high-level understanding and navigation\n",
    "- ‚úÖ **Book quotes** provide specific, verifiable evidence\n",
    "- ‚úÖ **Question rewriting** improves retrieval quality for ambiguous queries\n",
    "- ‚úÖ **Chain-of-thought reasoning** enhances answer quality for complex questions\n",
    "- ‚úÖ **Plan-and-execute agents** handle multi-hop reasoning effectively\n",
    "\n",
    "### 11.2 Areas for Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experimentation Ideas ---\n",
    "experimentation_ideas = \"\"\"\n",
    "üî¨ FUTURE EXPERIMENTS:\n",
    "\n",
    "1. CHUNK SIZE OPTIMIZATION\n",
    "   - Test chunk sizes: [500, 1000, 1500, 2000, 2500]\n",
    "   - Measure impact on retrieval quality and speed\n",
    "\n",
    "2. EMBEDDING MODEL COMPARISON\n",
    "   - Compare OpenAI embeddings vs. open-source alternatives\n",
    "   - Test multilingual embeddings for non-English questions\n",
    "\n",
    "3. RETRIEVAL PARAMETER TUNING\n",
    "   - Vary k-values: [1, 3, 5, 10, 15]\n",
    "   - Test different similarity thresholds\n",
    "\n",
    "4. ADVANCED TECHNIQUES\n",
    "   - Add reranking with cross-encoders\n",
    "   - Implement hybrid search (semantic + keyword)\n",
    "   - Try query expansion with generated synonyms\n",
    "   - Add contextual compression for long contexts\n",
    "\n",
    "5. MULTI-MODAL EXTENSIONS\n",
    "   - Add movie stills/images to vector store\n",
    "   - Include audio clips from audiobooks\n",
    "\n",
    "6. SCALABILITY TESTS\n",
    "   - Index all 7 Harry Potter books\n",
    "   - Test with larger document collections\n",
    "   - Measure latency vs. dataset size\n",
    "\"\"\"\n",
    "\n",
    "print(experimentation_ideas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Quick Experiment Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick Experiment: Chunk Size Comparison ---\n",
    "def experiment_chunk_sizes():\n",
    "    \"\"\"Quick template for testing different chunk sizes.\"\"\"\n",
    "    \n",
    "    chunk_sizes = [500, 1000, 1500, 2000]\n",
    "    results = {}\n",
    "    \n",
    "    test_question = \"What is the function of a Remembrall?\"\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        print(f\"\\nTesting chunk size: {size}\")\n",
    "        \n",
    "        # Create vector store with specific chunk size\n",
    "        # vector_store = encode_book(hp_pdf_path, chunk_size=size)\n",
    "        # retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "        \n",
    "        # For this demo, we'll simulate\n",
    "        answer = basic_rag_query(test_question)\n",
    "        results[size] = answer\n",
    "        print(f\"Answer: {answer[:100]}...\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Uncomment to run\n",
    "# chunk_experiment_results = experiment_chunk_sizes()\n",
    "# print(\"\\nChunk Size Experiment Results:\")\n",
    "# for size, answer in chunk_experiment_results.items():\n",
    "#     print(f\"{size}: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick Experiment: Model Comparison ---\n",
    "def experiment_different_llms():\n",
    "    \"\"\"Compare different LLMs for answering.\"\"\"\n",
    "    \n",
    "    models = {\n",
    "        \"gpt-3.5-turbo\": ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"),\n",
    "        \"gpt-4o\": ChatOpenAI(temperature=0, model_name=\"gpt-4o\"),\n",
    "        # \"llama3-70b\": ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key)\n",
    "    }\n",
    "    \n",
    "    question = \"Explain the significance of Harry's scar\"\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    for model_name, llm in models.items():\n",
    "        # Get context\n",
    "        docs = chunks_query_retriever.get_relevant_documents(question)\n",
    "        context = \" \".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        answer = llm.invoke(prompt).content\n",
    "        \n",
    "        print(f\"--- {model_name.upper()} ---\")\n",
    "        print(f\"{answer}\\n\")\n",
    "\n",
    "# Uncomment to run model comparison\n",
    "# experiment_different_llms()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
