{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production-Grade RAG: From Vanilla to Autonomous Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Challenge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Environment Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain langgraph langchain_openai chromadb beautifulsoup4 rank_bm25 lxml sentence-transformers cross-encoder ragas arxiv rich sec-api unstructured[html] tavily-python\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from getpass import getpass\n",
    "from pprint import pprint\n",
    "import uuid\n",
    "from typing import List, Dict, TypedDict, Literal, Optional\n",
    "\n",
    "# Set API keys securely\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass(f\"Enter your {var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Configure LangSmith\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"Advanced-Deep-Thinking-RAG-v2\"\n",
    "\n",
    "# Central config\n",
    "config = {\n",
    "    \"data_dir\": \"./data\",\n",
    "    \"vector_store_dir\": \"./vector_store\",\n",
    "    \"llm_provider\": \"openai\",\n",
    "    \"reasoning_llm\": \"gpt-4o\",\n",
    "    \"fast_llm\": \"gpt-4o-mini\",\n",
    "    \"embedding_model\": \"text-embedding-3-small\",\n",
    "    \"reranker_model\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    \"max_reasoning_iterations\": 7,\n",
    "    \"top_k_retrieval\": 10,\n",
    "    \"top_n_rerank\": 3,\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config[\"data_dir\"], exist_ok=True)\n",
    "os.makedirs(config[\"vector_store_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Environment setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download NVIDIA 10-K Filing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def download_and_parse_10k(url, doc_path_raw, doc_path_clean):\n",
    "    if os.path.exists(doc_path_clean):\n",
    "        print(f\"âœ“ Cleaned 10-K exists: {doc_path_clean}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Downloading 10-K from {url}...\")\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(doc_path_raw, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    \n",
    "    # Parse and clean HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    for table in soup.find_all('table'):\n",
    "        table.decompose()\n",
    "\n",
    "    text = ''\n",
    "    for p in soup.find_all(['p', 'div', 'span']):\n",
    "        text += p.get_text(strip=True) + '\\n\\n'\n",
    "    \n",
    "    clean_text = re.sub(r'\\n{3,}', '\\n\\n', text).strip()\n",
    "    clean_text = re.sub(r'\\s{2,}', ' ', clean_text).strip()\n",
    "    \n",
    "    with open(doc_path_clean, 'w', encoding='utf-8') as f:\n",
    "        f.write(clean_text)\n",
    "    print(f\"âœ“ Clean text saved: {doc_path_clean}\")\n",
    "\n",
    "# Download NVIDIA 2023 10-K\n",
    "url_10k = \"https://www.sec.gov/Archives/edgar/data/1045810/000104581023000017/nvda-20230129.htm\"\n",
    "doc_path_raw = os.path.join(config[\"data_dir\"], \"nvda_10k_2023_raw.html\")\n",
    "doc_path_clean = os.path.join(config[\"data_dir\"], \"nvda_10k_2023_clean.txt\")\n",
    "\n",
    "download_and_parse_10k(url_10k, doc_path_raw, doc_path_clean)\n",
    "\n",
    "with open(doc_path_clean, 'r', encoding='utf-8') as f:\n",
    "    print(\"Sample content:\", f.read(1000)[:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Challenge Query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_query = \"\"\"\n",
    "Based on NVIDIA's 2023 10-K filing, identify their key risks related to competition.\n",
    "Then, find recent news (post-filing, from 2024) about AMD's AI chip strategy and \n",
    "explain how this new strategy directly addresses or exacerbates one of NVIDIA's stated risks.\n",
    "\"\"\"\n",
    "print(\"Challenge query set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Baseline RAG Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Document Loading & Chunking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(doc_path_clean, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "doc_chunks = text_splitter.split_documents(documents)\n",
    "print(f\"âœ“ Split into {len(doc_chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Vector Store Creation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(model=config['embedding_model'])\n",
    "baseline_vector_store = Chroma.from_documents(\n",
    "    documents=doc_chunks,\n",
    "    embedding=embedding_function\n",
    ")\n",
    "baseline_retriever = baseline_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(f\"âœ“ Vector store created with {baseline_vector_store._collection.count()} embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Simple RAG Chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer based only on context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model=config[\"fast_llm\"], temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "baseline_rag_chain = (\n",
    "    {\"context\": baseline_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"âœ“ Baseline RAG chain ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Baseline Failure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "console = Console()\n",
    "print(\"Testing baseline RAG...\")\n",
    "baseline_result = baseline_rag_chain.invoke(complex_query)\n",
    "\n",
    "console.print(\"--- BASELINE OUTPUT ---\")\n",
    "console.print(Markdown(baseline_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced RAG Components\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 State Management\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Step(BaseModel):\n",
    "    sub_question: str\n",
    "    justification: str\n",
    "    tool: Literal[\"search_10k\", \"search_web\"]\n",
    "    keywords: List[str]\n",
    "    document_section: Optional[str]\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    steps: List[Step]\n",
    "\n",
    "class PastStep(TypedDict):\n",
    "    step_index: int\n",
    "    sub_question: str\n",
    "    retrieved_docs: List[Document]\n",
    "    summary: str\n",
    "\n",
    "class RAGState(TypedDict):\n",
    "    original_question: str\n",
    "    plan: Plan\n",
    "    past_steps: List[PastStep]\n",
    "    current_step_index: int\n",
    "    retrieved_docs: List[Document]\n",
    "    reranked_docs: List[Document]\n",
    "    synthesized_context: str\n",
    "    final_answer: str\n",
    "\n",
    "print(\"âœ“ State management classes defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tool-Aware Planner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a research planner. Decompose queries into steps and assign tools:\n",
    "    1. search_10k: For NVIDIA 10-K specific info\n",
    "    2. search_web: For recent news/external info\n",
    "    \n",
    "    For search_10k steps, suggest relevant 10-K sections.\"\"\"),\n",
    "    (\"human\", \"Query: {question}\")\n",
    "])\n",
    "\n",
    "reasoning_llm = ChatOpenAI(model=config[\"reasoning_llm\"], temperature=0)\n",
    "planner_agent = planner_prompt | reasoning_llm.with_structured_output(Plan)\n",
    "\n",
    "# Test planner\n",
    "test_plan = planner_agent.invoke({\"question\": complex_query})\n",
    "print(\"âœ“ Planner created\")\n",
    "print(f\"Plan has {len(test_plan.steps)} steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Query Rewriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "query_rewriter_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Optimize search queries for retrieval systems.\"),\n",
    "    (\"human\", \"\"\"Sub-question: {sub_question}\n",
    "Keywords: {keywords}\n",
    "Past context: {past_context}\n",
    "Rewritten query:\"\"\")\n",
    "])\n",
    "\n",
    "query_rewriter_agent = query_rewriter_prompt | reasoning_llm | StrOutputParser()\n",
    "print(\"âœ“ Query rewriter ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Add Metadata for Filtered Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding section metadata to chunks...\")\n",
    "section_pattern = r\"(ITEM\\s+\\d[A-Z]?\\.\\s*.*?)(?=\\nITEM\\s+\\d[A-Z]?\\.|$)\"\n",
    "raw_text = documents[0].page_content\n",
    "\n",
    "section_titles = re.findall(section_pattern, raw_text, re.IGNORECASE | re.DOTALL)\n",
    "section_titles = [title.strip().replace('\\n', ' ') for title in section_titles]\n",
    "\n",
    "sections_content = re.split(section_pattern, raw_text, flags=re.IGNORECASE | re.DOTALL)\n",
    "sections_content = [c.strip() for c in sections_content if c.strip() and not c.strip().lower().startswith('item ')]\n",
    "\n",
    "doc_chunks_with_metadata = []\n",
    "for i, content in enumerate(sections_content):\n",
    "    section_title = section_titles[i]\n",
    "    section_chunks = text_splitter.split_text(content)\n",
    "    for chunk in section_chunks:\n",
    "        doc_chunks_with_metadata.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"section\": section_title,\n",
    "                    \"source_doc\": doc_path_clean,\n",
    "                    \"id\": str(uuid.uuid4())\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"âœ“ Created {len(doc_chunks_with_metadata)} chunks with metadata\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Retrieval Supervisor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalDecision(BaseModel):\n",
    "    strategy: Literal[\"vector_search\", \"keyword_search\", \"hybrid_search\"]\n",
    "    justification: str\n",
    "\n",
    "retrieval_supervisor_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Choose best retrieval strategy:\n",
    "    1. vector_search: Conceptual/semantic queries\n",
    "    2. keyword_search: Specific terms/codes\n",
    "    3. hybrid_search: Default, combines both\"\"\"),\n",
    "    (\"human\", \"Query: {sub_question}\")\n",
    "])\n",
    "\n",
    "retrieval_supervisor_agent = retrieval_supervisor_prompt | reasoning_llm.with_structured_output(RetrievalDecision)\n",
    "print(\"âœ“ Retrieval supervisor ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Advanced Vector Store\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "advanced_vector_store = Chroma.from_documents(\n",
    "    documents=doc_chunks_with_metadata,\n",
    "    embedding=embedding_function\n",
    ")\n",
    "\n",
    "# Build BM25 index\n",
    "tokenized_corpus = [doc.page_content.split(\" \") for doc in doc_chunks_with_metadata]\n",
    "doc_ids = [doc.metadata[\"id\"] for doc in doc_chunks_with_metadata]\n",
    "doc_map = {doc.metadata[\"id\"]: doc for doc in doc_chunks_with_metadata}\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def vector_search_only(query: str, section_filter: str = None, k: int = 10):\n",
    "    filter_dict = {\"section\": section_filter} if section_filter and \"Unknown\" not in section_filter else None\n",
    "    return advanced_vector_store.similarity_search(query, k=k, filter=filter_dict)\n",
    "\n",
    "def bm25_search_only(query: str, k: int = 10):\n",
    "    tokenized_query = query.split(\" \")\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    top_k_indices = np.argsort(bm25_scores)[::-1][:k]\n",
    "    return [doc_map[doc_ids[i]] for i in top_k_indices]\n",
    "\n",
    "def hybrid_search(query: str, section_filter: str = None, k: int = 10):\n",
    "    bm25_docs = bm25_search_only(query, k=k)\n",
    "    semantic_docs = vector_search_only(query, section_filter=section_filter, k=k)\n",
    "    \n",
    "    # Reciprocal Rank Fusion\n",
    "    all_docs = {doc.metadata[\"id\"]: doc for doc in bm25_docs + semantic_docs}.values()\n",
    "    ranked_lists = [\n",
    "        [doc.metadata[\"id\"] for doc in bm25_docs],\n",
    "        [doc.metadata[\"id\"] for doc in semantic_docs]\n",
    "    ]\n",
    "    \n",
    "    rrf_scores = {}\n",
    "    for doc_list in ranked_lists:\n",
    "        for i, doc_id in enumerate(doc_list):\n",
    "            if doc_id not in rrf_scores:\n",
    "                rrf_scores[doc_id] = 0\n",
    "            rrf_scores[doc_id] += 1 / (i + 61)\n",
    "    \n",
    "    sorted_doc_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)\n",
    "    return [doc_map[doc_id] for doc_id in sorted_doc_ids[:k]]\n",
    "\n",
    "print(\"âœ“ Advanced retrieval functions ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Reranker\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder(config[\"reranker_model\"])\n",
    "\n",
    "def rerank_documents_function(query: str, documents: List[Document]) -> List[Document]:\n",
    "    if not documents:\n",
    "        return []\n",
    "    pairs = [(query, doc.page_content) for doc in documents]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    doc_scores = list(zip(documents, scores))\n",
    "    doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [doc for doc, score in doc_scores[:config[\"top_n_rerank\"]]]\n",
    "\n",
    "print(\"âœ“ Reranker ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Context Distiller\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Synthesize retrieved documents into concise context.\"),\n",
    "    (\"human\", \"Question: {question}\\nDocuments:\\n{context}\")\n",
    "])\n",
    "\n",
    "distiller_agent = distiller_prompt | reasoning_llm | StrOutputParser()\n",
    "print(\"âœ“ Distiller ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Web Search Tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "def web_search_function(query: str) -> List[Document]:\n",
    "    results = web_search_tool.invoke({\"query\": query})\n",
    "    return [Document(page_content=res[\"content\"], metadata={\"source\": res[\"url\"]}) for res in results]\n",
    "\n",
    "print(\"âœ“ Web search ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Reflection & Policy Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflection agent\n",
    "reflection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Write one-sentence summary of findings.\"),\n",
    "    (\"human\", \"Sub-question: {sub_question}\\nContext:\\n{context}\")\n",
    "])\n",
    "reflection_agent = reflection_prompt | reasoning_llm | StrOutputParser()\n",
    "\n",
    "# Policy agent\n",
    "class Decision(BaseModel):\n",
    "    next_action: Literal[\"CONTINUE_PLAN\", \"FINISH\"]\n",
    "    justification: str\n",
    "\n",
    "policy_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Decide next action based on research progress:\n",
    "    - FINISH if question answered\n",
    "    - CONTINUE_PLAN if more info needed\"\"\"),\n",
    "    (\"human\", \"Question: {question}\\nPlan:\\n{plan}\\nHistory:\\n{history}\")\n",
    "])\n",
    "policy_agent = policy_prompt | reasoning_llm.with_structured_output(Decision)\n",
    "\n",
    "print(\"âœ“ Reflection & policy agents ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: LangGraph Orchestration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Graph Nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def get_past_context_str(past_steps: List[PastStep]) -> str:\n",
    "    return \"\\n\\n\".join([f\"Step {s['step_index']}: {s['sub_question']}\\nSummary: {s['summary']}\" for s in past_steps])\n",
    "\n",
    "# Define nodes\n",
    "def plan_node(state: RAGState) -> Dict:\n",
    "    print(\"ðŸ§  Generating plan...\")\n",
    "    plan = planner_agent.invoke({\"question\": state[\"original_question\"]})\n",
    "    return {\"plan\": plan, \"current_step_index\": 0, \"past_steps\": []}\n",
    "\n",
    "def retrieval_node(state: RAGState) -> Dict:\n",
    "    current_step = state[\"plan\"].steps[state[\"current_step_index\"]]\n",
    "    print(f\"ðŸ” Retrieving (Step {state['current_step_index'] + 1})...\")\n",
    "    \n",
    "    past_context = get_past_context_str(state['past_steps'])\n",
    "    rewritten_query = query_rewriter_agent.invoke({\n",
    "        \"sub_question\": current_step.sub_question,\n",
    "        \"keywords\": current_step.keywords,\n",
    "        \"past_context\": past_context\n",
    "    })\n",
    "    \n",
    "    retrieval_decision = retrieval_supervisor_agent.invoke({\"sub_question\": rewritten_query})\n",
    "    \n",
    "    if retrieval_decision.strategy == 'vector_search':\n",
    "        docs = vector_search_only(rewritten_query, current_step.document_section, config['top_k_retrieval'])\n",
    "    elif retrieval_decision.strategy == 'keyword_search':\n",
    "        docs = bm25_search_only(rewritten_query, config['top_k_retrieval'])\n",
    "    else:\n",
    "        docs = hybrid_search(rewritten_query, current_step.document_section, config['top_k_retrieval'])\n",
    "    \n",
    "    return {\"retrieved_docs\": docs}\n",
    "\n",
    "def web_search_node(state: RAGState) -> Dict:\n",
    "    current_step = state[\"plan\"].steps[state[\"current_step_index\"]]\n",
    "    print(f\"ðŸŒ Web search (Step {state['current_step_index'] + 1})...\")\n",
    "    \n",
    "    past_context = get_past_context_str(state['past_steps'])\n",
    "    rewritten_query = query_rewriter_agent.invoke({\n",
    "        \"sub_question\": current_step.sub_question,\n",
    "        \"keywords\": current_step.keywords,\n",
    "        \"past_context\": past_context\n",
    "    })\n",
    "    \n",
    "    return {\"retrieved_docs\": web_search_function(rewritten_query)}\n",
    "\n",
    "def rerank_node(state: RAGState) -> Dict:\n",
    "    print(\"ðŸŽ¯ Reranking...\")\n",
    "    current_step = state[\"plan\"].steps[state[\"current_step_index\"]]\n",
    "    return {\"reranked_docs\": rerank_documents_function(current_step.sub_question, state[\"retrieved_docs\"])}\n",
    "\n",
    "def compression_node(state: RAGState) -> Dict:\n",
    "    print(\"âœ‚ï¸ Distilling context...\")\n",
    "    current_step = state[\"plan\"].steps[state[\"current_step_index\"]]\n",
    "    context = format_docs(state[\"reranked_docs\"])\n",
    "    synthesized = distiller_agent.invoke({\"question\": current_step.sub_question, \"context\": context})\n",
    "    return {\"synthesized_context\": synthesized}\n",
    "\n",
    "def reflection_node(state: RAGState) -> Dict:\n",
    "    print(\"ðŸ¤” Reflecting...\")\n",
    "    current_step = state[\"plan\"].steps[state[\"current_step_index\"]]\n",
    "    summary = reflection_agent.invoke({\n",
    "        \"sub_question\": current_step.sub_question,\n",
    "        \"context\": state['synthesized_context']\n",
    "    })\n",
    "    \n",
    "    new_step = {\n",
    "        \"step_index\": state[\"current_step_index\"] + 1,\n",
    "        \"sub_question\": current_step.sub_question,\n",
    "        \"retrieved_docs\": state['reranked_docs'],\n",
    "        \"summary\": summary\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"past_steps\": state[\"past_steps\"] + [new_step],\n",
    "        \"current_step_index\": state[\"current_step_index\"] + 1\n",
    "    }\n",
    "\n",
    "def final_answer_node(state: RAGState) -> Dict:\n",
    "    print(\"âœ… Generating final answer...\")\n",
    "    \n",
    "    final_context = \"\"\n",
    "    for i, step in enumerate(state['past_steps']):\n",
    "        final_context += f\"\\n--- Step {i+1} ---\\n\"\n",
    "        for doc in step['retrieved_docs']:\n",
    "            source = doc.metadata.get('section') or doc.metadata.get('source')\n",
    "            final_context += f\"Source: {source}\\nContent: {doc.page_content}\\n\\n\"\n",
    "    \n",
    "    final_answer_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Synthesize research into answer with citations.\"),\n",
    "        (\"human\", \"Question: {question}\\nContext:\\n{context}\")\n",
    "    ])\n",
    "    \n",
    "    final_answer_agent = final_answer_prompt | reasoning_llm | StrOutputParser()\n",
    "    answer = final_answer_agent.invoke({\n",
    "        \"question\": state['original_question'],\n",
    "        \"context\": final_context\n",
    "    })\n",
    "    \n",
    "    return {\"final_answer\": answer}\n",
    "\n",
    "print(\"âœ“ Graph nodes defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Conditional Edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_by_tool(state: RAGState) -> str:\n",
    "    current_step = state[\"plan\"].steps[state[\"current_step_index\"]]\n",
    "    return current_step.tool\n",
    "\n",
    "def should_continue_node(state: RAGState) -> str:\n",
    "    print(\"ðŸš¦ Evaluating policy...\")\n",
    "    \n",
    "    if state[\"current_step_index\"] >= len(state[\"plan\"].steps):\n",
    "        print(\"  â†’ Plan complete\")\n",
    "        return \"finish\"\n",
    "    \n",
    "    if state[\"current_step_index\"] >= config[\"max_reasoning_iterations\"]:\n",
    "        print(\"  â†’ Max iterations\")\n",
    "        return \"finish\"\n",
    "    \n",
    "    if not state[\"reranked_docs\"]:\n",
    "        print(\"  â†’ No documents found\")\n",
    "        return \"continue\"\n",
    "    \n",
    "    history = get_past_context_str(state['past_steps'])\n",
    "    plan_str = json.dumps([s.dict() for s in state['plan'].steps])\n",
    "    decision = policy_agent.invoke({\n",
    "        \"question\": state[\"original_question\"],\n",
    "        \"plan\": plan_str,\n",
    "        \"history\": history\n",
    "    })\n",
    "    \n",
    "    print(f\"  â†’ Decision: {decision.next_action}\")\n",
    "    return \"finish\" if decision.next_action == \"FINISH\" else \"continue\"\n",
    "\n",
    "print(\"âœ“ Edge logic defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Build Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(RAGState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"plan\", plan_node)\n",
    "graph.add_node(\"retrieve_10k\", retrieval_node)\n",
    "graph.add_node(\"retrieve_web\", web_search_node)\n",
    "graph.add_node(\"rerank\", rerank_node)\n",
    "graph.add_node(\"compress\", compression_node)\n",
    "graph.add_node(\"reflect\", reflection_node)\n",
    "graph.add_node(\"generate_final_answer\", final_answer_node)\n",
    "\n",
    "# Define edges\n",
    "graph.set_entry_point(\"plan\")\n",
    "graph.add_conditional_edges(\n",
    "    \"plan\",\n",
    "    route_by_tool,\n",
    "    {\"search_10k\": \"retrieve_10k\", \"search_web\": \"retrieve_web\"},\n",
    ")\n",
    "graph.add_edge(\"retrieve_10k\", \"rerank\")\n",
    "graph.add_edge(\"retrieve_web\", \"rerank\")\n",
    "graph.add_edge(\"rerank\", \"compress\")\n",
    "graph.add_edge(\"compress\", \"reflect\")\n",
    "graph.add_conditional_edges(\n",
    "    \"reflect\",\n",
    "    should_continue_node,\n",
    "    {\"continue\": \"plan\", \"finish\": \"generate_final_answer\"},\n",
    ")\n",
    "graph.add_edge(\"generate_final_answer\", END)\n",
    "\n",
    "deep_thinking_rag_graph = graph.compile()\n",
    "print(\"âœ“ Graph compiled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Visualize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    png_image = deep_thinking_rag_graph.get_graph().draw_png()\n",
    "    display(Image(png_image))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Run Advanced Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Execute\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Starting advanced RAG...\")\n",
    "graph_input = {\"original_question\": complex_query}\n",
    "final_state = None\n",
    "\n",
    "for chunk in deep_thinking_rag_graph.stream(graph_input, stream_mode=\"values\"):\n",
    "    final_state = chunk\n",
    "    print(f\"Step completed\")\n",
    "\n",
    "print(\"\\nâœ… Graph execution complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Show Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\"--- ADVANCED RAG OUTPUT ---\")\n",
    "console.print(Markdown(final_state['final_answer']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Setup Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import context_precision, context_recall, faithfulness, answer_correctness\n",
    "import pandas as pd\n",
    "\n",
    "ground_truth = \"\"\"\n",
    "NVIDIA's 2023 10-K lists intense competition and rapid technological change as key risks.\n",
    "This risk is exacerbated by AMD's 2024 strategy, specifically the MI300X AI accelerator,\n",
    "which directly competes with NVIDIA's H100 and has been adopted by major cloud providers.\n",
    "\"\"\"\n",
    "\n",
    "# Get contexts for evaluation\n",
    "retrieved_docs_baseline = baseline_retriever.invoke(complex_query)\n",
    "baseline_contexts = [[doc.page_content for doc in retrieved_docs_baseline]]\n",
    "\n",
    "advanced_contexts_flat = []\n",
    "for step in final_state['past_steps']:\n",
    "    advanced_contexts_flat.extend([doc.page_content for doc in step['retrieved_docs']])\n",
    "advanced_contexts = [list(set(advanced_contexts_flat))]\n",
    "\n",
    "eval_data = {\n",
    "    'question': [complex_query, complex_query],\n",
    "    'answer': [baseline_result, final_state['final_answer']],\n",
    "    'contexts': baseline_contexts + advanced_contexts,\n",
    "    'ground_truth': [ground_truth, ground_truth]\n",
    "}\n",
    "\n",
    "eval_dataset = Dataset.from_dict(eval_data)\n",
    "print(\"âœ“ Evaluation dataset ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Run Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [context_precision, context_recall, faithfulness, answer_correctness]\n",
    "result = evaluate(eval_dataset, metrics=metrics, is_async=False)\n",
    "\n",
    "results_df = result.to_pandas()\n",
    "results_df.index = ['baseline', 'advanced']\n",
    "print(\"\\nðŸ“Š Evaluation Results:\")\n",
    "print(results_df[['context_precision', 'context_recall', 'faithfulness', 'answer_correctness']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Production Optimizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Caching\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# Enable caching for production\n",
    "set_llm_cache(InMemoryCache())\n",
    "print(\"âœ“ LLM caching enabled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Provenance Tracking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already implemented in final_answer_node\n",
    "print(\"âœ“ Citations and provenance built-in\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}